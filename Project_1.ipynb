{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BQZ3yvF3tvqI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "9e1f4839-e780-4b65-eedc-b843e96e410e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               id             date      price  bedrooms  bathrooms  \\\n",
              "9469   2422049107  20140508T000000   350000.0       4.0       1.75   \n",
              "15048  6151800225  20150409T000000   475000.0       3.0       1.75   \n",
              "3728   2919701105  20141209T000000   422000.0       2.0       1.75   \n",
              "2552   4137060270  20150105T000000   313000.0       4.0       2.50   \n",
              "3275    818500490  20141009T000000   153503.0       2.0       2.50   \n",
              "...           ...              ...        ...       ...        ...   \n",
              "198    2824079053  20150113T000000   440000.0       3.0       2.50   \n",
              "12076  2193310320  20150306T000000   595000.0       4.0       2.50   \n",
              "8709   7504010750  20140924T000000   649990.0       4.0       2.25   \n",
              "18794   126059310  20141130T000000  1000000.0       3.0       2.25   \n",
              "676    7846700310  20140623T000000   280000.0       2.0       1.00   \n",
              "\n",
              "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
              "9469          2250     13515     1.0           0     0  ...      8   \n",
              "15048         1850     26445     1.0           0     0  ...      7   \n",
              "3728          1320      2609     1.0           0     0  ...      7   \n",
              "2552          2460     10320     2.0           0     0  ...      8   \n",
              "3275          1240      3649     2.0           0     0  ...      7   \n",
              "...            ...       ...     ...         ...   ...  ...    ...   \n",
              "198           1910     66211     2.0           0     0  ...      7   \n",
              "12076         2330      7064     1.0           0     0  ...      8   \n",
              "8709          2130     11900     2.0           0     0  ...      9   \n",
              "18794         3040     52302     1.0           0     0  ...      9   \n",
              "676           1010      3000     1.0           0     0  ...      7   \n",
              "\n",
              "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
              "9469         2150            100      1940             0    98030  47.3789   \n",
              "15048        1850              0      1962          1977    98010  47.3412   \n",
              "3728          920            400      1938             0    98117  47.6878   \n",
              "2552         2460              0      1993             0    98092  47.2599   \n",
              "3275         1240              0      1986             0    98003  47.3241   \n",
              "...           ...            ...       ...           ...      ...      ...   \n",
              "198          1910              0      1997             0    98024  47.5385   \n",
              "12076        1780            550      1984             0    98052  47.6955   \n",
              "8709         2130              0      1976             0    98074  47.6408   \n",
              "18794        3040              0      2005             0    98072  47.7635   \n",
              "676          1010              0      1925             0    98045  47.4965   \n",
              "\n",
              "          long  sqft_living15  sqft_lot15  \n",
              "9469  -122.229           2150       12508  \n",
              "15048 -122.051           2110       23280  \n",
              "3728  -122.366           1200        4220  \n",
              "2552  -122.215           2210        9024  \n",
              "3275  -122.322           1400        3721  \n",
              "...        ...            ...         ...  \n",
              "198   -121.911           2330       67268  \n",
              "12076 -122.097           1740        8075  \n",
              "8709  -122.058           2590       11900  \n",
              "18794 -122.112           2070       38600  \n",
              "676   -121.785           1150        7000  \n",
              "\n",
              "[16200 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b573570-b3c3-4ebb-94ae-ee54f1ceaa7b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>...</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9469</th>\n",
              "      <td>2422049107</td>\n",
              "      <td>20140508T000000</td>\n",
              "      <td>350000.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.75</td>\n",
              "      <td>2250</td>\n",
              "      <td>13515</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>8</td>\n",
              "      <td>2150</td>\n",
              "      <td>100</td>\n",
              "      <td>1940</td>\n",
              "      <td>0</td>\n",
              "      <td>98030</td>\n",
              "      <td>47.3789</td>\n",
              "      <td>-122.229</td>\n",
              "      <td>2150</td>\n",
              "      <td>12508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15048</th>\n",
              "      <td>6151800225</td>\n",
              "      <td>20150409T000000</td>\n",
              "      <td>475000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1850</td>\n",
              "      <td>26445</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1850</td>\n",
              "      <td>0</td>\n",
              "      <td>1962</td>\n",
              "      <td>1977</td>\n",
              "      <td>98010</td>\n",
              "      <td>47.3412</td>\n",
              "      <td>-122.051</td>\n",
              "      <td>2110</td>\n",
              "      <td>23280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3728</th>\n",
              "      <td>2919701105</td>\n",
              "      <td>20141209T000000</td>\n",
              "      <td>422000.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1320</td>\n",
              "      <td>2609</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>920</td>\n",
              "      <td>400</td>\n",
              "      <td>1938</td>\n",
              "      <td>0</td>\n",
              "      <td>98117</td>\n",
              "      <td>47.6878</td>\n",
              "      <td>-122.366</td>\n",
              "      <td>1200</td>\n",
              "      <td>4220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2552</th>\n",
              "      <td>4137060270</td>\n",
              "      <td>20150105T000000</td>\n",
              "      <td>313000.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.50</td>\n",
              "      <td>2460</td>\n",
              "      <td>10320</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>8</td>\n",
              "      <td>2460</td>\n",
              "      <td>0</td>\n",
              "      <td>1993</td>\n",
              "      <td>0</td>\n",
              "      <td>98092</td>\n",
              "      <td>47.2599</td>\n",
              "      <td>-122.215</td>\n",
              "      <td>2210</td>\n",
              "      <td>9024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3275</th>\n",
              "      <td>818500490</td>\n",
              "      <td>20141009T000000</td>\n",
              "      <td>153503.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1240</td>\n",
              "      <td>3649</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1240</td>\n",
              "      <td>0</td>\n",
              "      <td>1986</td>\n",
              "      <td>0</td>\n",
              "      <td>98003</td>\n",
              "      <td>47.3241</td>\n",
              "      <td>-122.322</td>\n",
              "      <td>1400</td>\n",
              "      <td>3721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>2824079053</td>\n",
              "      <td>20150113T000000</td>\n",
              "      <td>440000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1910</td>\n",
              "      <td>66211</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1910</td>\n",
              "      <td>0</td>\n",
              "      <td>1997</td>\n",
              "      <td>0</td>\n",
              "      <td>98024</td>\n",
              "      <td>47.5385</td>\n",
              "      <td>-121.911</td>\n",
              "      <td>2330</td>\n",
              "      <td>67268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12076</th>\n",
              "      <td>2193310320</td>\n",
              "      <td>20150306T000000</td>\n",
              "      <td>595000.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.50</td>\n",
              "      <td>2330</td>\n",
              "      <td>7064</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>8</td>\n",
              "      <td>1780</td>\n",
              "      <td>550</td>\n",
              "      <td>1984</td>\n",
              "      <td>0</td>\n",
              "      <td>98052</td>\n",
              "      <td>47.6955</td>\n",
              "      <td>-122.097</td>\n",
              "      <td>1740</td>\n",
              "      <td>8075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8709</th>\n",
              "      <td>7504010750</td>\n",
              "      <td>20140924T000000</td>\n",
              "      <td>649990.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2130</td>\n",
              "      <td>11900</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>9</td>\n",
              "      <td>2130</td>\n",
              "      <td>0</td>\n",
              "      <td>1976</td>\n",
              "      <td>0</td>\n",
              "      <td>98074</td>\n",
              "      <td>47.6408</td>\n",
              "      <td>-122.058</td>\n",
              "      <td>2590</td>\n",
              "      <td>11900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18794</th>\n",
              "      <td>126059310</td>\n",
              "      <td>20141130T000000</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.25</td>\n",
              "      <td>3040</td>\n",
              "      <td>52302</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>9</td>\n",
              "      <td>3040</td>\n",
              "      <td>0</td>\n",
              "      <td>2005</td>\n",
              "      <td>0</td>\n",
              "      <td>98072</td>\n",
              "      <td>47.7635</td>\n",
              "      <td>-122.112</td>\n",
              "      <td>2070</td>\n",
              "      <td>38600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>676</th>\n",
              "      <td>7846700310</td>\n",
              "      <td>20140623T000000</td>\n",
              "      <td>280000.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1010</td>\n",
              "      <td>3000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1010</td>\n",
              "      <td>0</td>\n",
              "      <td>1925</td>\n",
              "      <td>0</td>\n",
              "      <td>98045</td>\n",
              "      <td>47.4965</td>\n",
              "      <td>-121.785</td>\n",
              "      <td>1150</td>\n",
              "      <td>7000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16200 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b573570-b3c3-4ebb-94ae-ee54f1ceaa7b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b573570-b3c3-4ebb-94ae-ee54f1ceaa7b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b573570-b3c3-4ebb-94ae-ee54f1ceaa7b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import math\n",
        "\n",
        "df=pd.read_csv('house_data_train.csv', index_col = 0)\n",
        "df = df.dropna()\n",
        "df.columns\n",
        "df.head(16200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'sqft_living', 'floors', 'waterfront', 'view', 'grade', 'sqft_above', 'sqft_basement']\n",
        "\n",
        "X=df[features]\n",
        "Y=df['price']\n",
        "\n",
        "W=Y.std()\n",
        "\n",
        "Y=np.array((Y-Y.mean())/Y.std())\n",
        "\n",
        "M=X.mean()\n",
        "X=X.apply(lambda rec:(rec-rec.mean())/rec.std(),axis=0)\n",
        "M"
      ],
      "metadata": {
        "id": "MJNlHzhW8aHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d25769a-d486-4f65-b2be-876aaa5f7887"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bedrooms             3.367469\n",
              "bathrooms            2.112701\n",
              "sqft_living       2081.468580\n",
              "sqft_lot         14987.381420\n",
              "floors               1.492346\n",
              "sqft_living       2081.468580\n",
              "floors               1.492346\n",
              "waterfront           0.007963\n",
              "view                 0.237222\n",
              "grade                7.656111\n",
              "sqft_above        1789.841728\n",
              "sqft_basement      291.626852\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Split Data"
      ],
      "metadata": {
        "id": "-7roNoHAHBsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, shuffle=False)\n",
        "\n",
        "kfold = KFold(n_splits = 10, shuffle = True, random_state = 100)\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "cv_scores = cross_val_score(lr, X_train, Y_train, cv = kfold)"
      ],
      "metadata": {
        "id": "oNJoQ1X6Ffsz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Linear Regression"
      ],
      "metadata": {
        "id": "HusAsBa3JNZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize(dim):\n",
        "    b=random.random()\n",
        "    a=np.random.rand(dim)\n",
        "    return b,a\n",
        "\n",
        "def predict_Y(b,a,X):\n",
        "    return b + np.dot(X,a)\n",
        "\n",
        "def get_cost(Y,Y_hat):\n",
        "    Y_resd=Y-Y_hat\n",
        "    return np.sum(np.dot(Y_resd.T,Y_resd))/len(Y-Y_resd)\n",
        "\n",
        "def update_theta(x,y,y_hat,b_0,theta_o,learning_rate):\n",
        "    db=(np.sum(y_hat-y)*2)/len(y)\n",
        "    dw=(np.dot((y_hat-y),x)*2)/len(y)\n",
        "    b_1=b_0-learning_rate*db\n",
        "    theta_1=theta_o-learning_rate*dw\n",
        "    return b_1,theta_1\n",
        "\n",
        "def run_gradient_descent(X,Y,alpha,num_iterations):\n",
        "    tolerance = 1e-8\n",
        "    b,theta=initialize(X.shape[1])\n",
        "    num_weight = len(features)\n",
        "    weights = [f'weights_a{i}' for i in range(num_weight)]\n",
        "    gd_iterations_df=pd.DataFrame(columns=['iteration','value_b', *weights, 'cost'])\n",
        "    result_idx=0\n",
        "    prev_cost = 0\n",
        "    for iter_num in range(num_iterations):\n",
        "        Y_hat=predict_Y(b,theta,X)\n",
        "        this_cost=get_cost(Y,Y_hat)\n",
        "        prev_b=b\n",
        "        prev_theta=theta\n",
        "        b,theta=update_theta(X,Y,Y_hat,prev_b,prev_theta,alpha)\n",
        "        gd_iterations_df.loc[result_idx]=[iter_num,b, *theta[:num_weight] , this_cost]\n",
        "        result_idx=result_idx+1   \n",
        "        if (np.abs(this_cost - prev_cost)<tolerance):\n",
        "          print(np.abs(this_cost - prev_cost))\n",
        "          break\n",
        "        prev_cost = this_cost\n",
        "    return gd_iterations_df,b,theta\n",
        "        \n",
        "gd_iterations_df,b,theta=run_gradient_descent(X,Y,alpha=0.01,num_iterations=1000)\n",
        "#gd_iterations_df.head(5)\n",
        "gd_iterations_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "1YAJgog_JMsm",
        "outputId": "6b1a5a2a-116f-4278-9d9b-83093b1a8443"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.949880530601973e-09\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     iteration       value_b  weights_a0  weights_a1  weights_a2  weights_a3  \\\n",
              "0          0.0  2.660030e-02    0.262784    0.841381    0.856773    0.158240   \n",
              "1          1.0  2.606830e-02    0.223598    0.785642    0.799546    0.146696   \n",
              "2          2.0  2.554693e-02    0.188451    0.735503    0.748439    0.136310   \n",
              "3          3.0  2.503599e-02    0.156917    0.690372    0.702800    0.126961   \n",
              "4          4.0  2.453527e-02    0.128617    0.649723    0.662050    0.118536   \n",
              "..         ...           ...         ...         ...         ...         ...   \n",
              "958      958.0  1.045844e-10   -0.093331   -0.032560    0.351279   -0.047834   \n",
              "959      959.0  1.024927e-10   -0.093328   -0.032566    0.351278   -0.047834   \n",
              "960      960.0  1.004428e-10   -0.093325   -0.032572    0.351277   -0.047833   \n",
              "961      961.0  9.843398e-11   -0.093322   -0.032579    0.351277   -0.047833   \n",
              "962      962.0  9.646530e-11   -0.093319   -0.032585    0.351276   -0.047832   \n",
              "\n",
              "     weights_a4  weights_a5  weights_a6  weights_a7  weights_a8  weights_a9  \\\n",
              "0      0.726472    0.532180    0.247871    0.136924    0.146727    0.451285   \n",
              "1      0.687715    0.474953    0.209115    0.131585    0.131149    0.402958   \n",
              "2      0.652903    0.423846    0.174302    0.127008    0.117455    0.360054   \n",
              "3      0.621617    0.378207    0.143016    0.123105    0.105439    0.321994   \n",
              "4      0.593485    0.337457    0.114884    0.119800    0.094920    0.288260   \n",
              "..          ...         ...         ...         ...         ...         ...   \n",
              "958    0.227640    0.026686   -0.250960    0.138773    0.124917    0.307362   \n",
              "959    0.227641    0.026685   -0.250959    0.138773    0.124916    0.307368   \n",
              "960    0.227642    0.026684   -0.250958    0.138774    0.124915    0.307375   \n",
              "961    0.227643    0.026684   -0.250958    0.138774    0.124914    0.307382   \n",
              "962    0.227644    0.026683   -0.250957    0.138775    0.124914    0.307388   \n",
              "\n",
              "     weights_a10  weights_a11       cost  \n",
              "0       0.384314     0.629228  12.525544  \n",
              "1       0.332604     0.607346  10.145805  \n",
              "2       0.286705     0.587275   8.243143  \n",
              "3       0.245996     0.568830   6.721654  \n",
              "4       0.209920     0.551848   5.504722  \n",
              "..           ...          ...        ...  \n",
              "958     0.104782     0.103652   0.404495  \n",
              "959     0.104780     0.103653   0.404495  \n",
              "960     0.104778     0.103655   0.404495  \n",
              "961     0.104777     0.103656   0.404495  \n",
              "962     0.104775     0.103658   0.404495  \n",
              "\n",
              "[963 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9bd32b6-e19e-49d6-88ad-86ec44bb3ec0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iteration</th>\n",
              "      <th>value_b</th>\n",
              "      <th>weights_a0</th>\n",
              "      <th>weights_a1</th>\n",
              "      <th>weights_a2</th>\n",
              "      <th>weights_a3</th>\n",
              "      <th>weights_a4</th>\n",
              "      <th>weights_a5</th>\n",
              "      <th>weights_a6</th>\n",
              "      <th>weights_a7</th>\n",
              "      <th>weights_a8</th>\n",
              "      <th>weights_a9</th>\n",
              "      <th>weights_a10</th>\n",
              "      <th>weights_a11</th>\n",
              "      <th>cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.660030e-02</td>\n",
              "      <td>0.262784</td>\n",
              "      <td>0.841381</td>\n",
              "      <td>0.856773</td>\n",
              "      <td>0.158240</td>\n",
              "      <td>0.726472</td>\n",
              "      <td>0.532180</td>\n",
              "      <td>0.247871</td>\n",
              "      <td>0.136924</td>\n",
              "      <td>0.146727</td>\n",
              "      <td>0.451285</td>\n",
              "      <td>0.384314</td>\n",
              "      <td>0.629228</td>\n",
              "      <td>12.525544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2.606830e-02</td>\n",
              "      <td>0.223598</td>\n",
              "      <td>0.785642</td>\n",
              "      <td>0.799546</td>\n",
              "      <td>0.146696</td>\n",
              "      <td>0.687715</td>\n",
              "      <td>0.474953</td>\n",
              "      <td>0.209115</td>\n",
              "      <td>0.131585</td>\n",
              "      <td>0.131149</td>\n",
              "      <td>0.402958</td>\n",
              "      <td>0.332604</td>\n",
              "      <td>0.607346</td>\n",
              "      <td>10.145805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2.554693e-02</td>\n",
              "      <td>0.188451</td>\n",
              "      <td>0.735503</td>\n",
              "      <td>0.748439</td>\n",
              "      <td>0.136310</td>\n",
              "      <td>0.652903</td>\n",
              "      <td>0.423846</td>\n",
              "      <td>0.174302</td>\n",
              "      <td>0.127008</td>\n",
              "      <td>0.117455</td>\n",
              "      <td>0.360054</td>\n",
              "      <td>0.286705</td>\n",
              "      <td>0.587275</td>\n",
              "      <td>8.243143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2.503599e-02</td>\n",
              "      <td>0.156917</td>\n",
              "      <td>0.690372</td>\n",
              "      <td>0.702800</td>\n",
              "      <td>0.126961</td>\n",
              "      <td>0.621617</td>\n",
              "      <td>0.378207</td>\n",
              "      <td>0.143016</td>\n",
              "      <td>0.123105</td>\n",
              "      <td>0.105439</td>\n",
              "      <td>0.321994</td>\n",
              "      <td>0.245996</td>\n",
              "      <td>0.568830</td>\n",
              "      <td>6.721654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>2.453527e-02</td>\n",
              "      <td>0.128617</td>\n",
              "      <td>0.649723</td>\n",
              "      <td>0.662050</td>\n",
              "      <td>0.118536</td>\n",
              "      <td>0.593485</td>\n",
              "      <td>0.337457</td>\n",
              "      <td>0.114884</td>\n",
              "      <td>0.119800</td>\n",
              "      <td>0.094920</td>\n",
              "      <td>0.288260</td>\n",
              "      <td>0.209920</td>\n",
              "      <td>0.551848</td>\n",
              "      <td>5.504722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>958</th>\n",
              "      <td>958.0</td>\n",
              "      <td>1.045844e-10</td>\n",
              "      <td>-0.093331</td>\n",
              "      <td>-0.032560</td>\n",
              "      <td>0.351279</td>\n",
              "      <td>-0.047834</td>\n",
              "      <td>0.227640</td>\n",
              "      <td>0.026686</td>\n",
              "      <td>-0.250960</td>\n",
              "      <td>0.138773</td>\n",
              "      <td>0.124917</td>\n",
              "      <td>0.307362</td>\n",
              "      <td>0.104782</td>\n",
              "      <td>0.103652</td>\n",
              "      <td>0.404495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>959</th>\n",
              "      <td>959.0</td>\n",
              "      <td>1.024927e-10</td>\n",
              "      <td>-0.093328</td>\n",
              "      <td>-0.032566</td>\n",
              "      <td>0.351278</td>\n",
              "      <td>-0.047834</td>\n",
              "      <td>0.227641</td>\n",
              "      <td>0.026685</td>\n",
              "      <td>-0.250959</td>\n",
              "      <td>0.138773</td>\n",
              "      <td>0.124916</td>\n",
              "      <td>0.307368</td>\n",
              "      <td>0.104780</td>\n",
              "      <td>0.103653</td>\n",
              "      <td>0.404495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>960</th>\n",
              "      <td>960.0</td>\n",
              "      <td>1.004428e-10</td>\n",
              "      <td>-0.093325</td>\n",
              "      <td>-0.032572</td>\n",
              "      <td>0.351277</td>\n",
              "      <td>-0.047833</td>\n",
              "      <td>0.227642</td>\n",
              "      <td>0.026684</td>\n",
              "      <td>-0.250958</td>\n",
              "      <td>0.138774</td>\n",
              "      <td>0.124915</td>\n",
              "      <td>0.307375</td>\n",
              "      <td>0.104778</td>\n",
              "      <td>0.103655</td>\n",
              "      <td>0.404495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>961</th>\n",
              "      <td>961.0</td>\n",
              "      <td>9.843398e-11</td>\n",
              "      <td>-0.093322</td>\n",
              "      <td>-0.032579</td>\n",
              "      <td>0.351277</td>\n",
              "      <td>-0.047833</td>\n",
              "      <td>0.227643</td>\n",
              "      <td>0.026684</td>\n",
              "      <td>-0.250958</td>\n",
              "      <td>0.138774</td>\n",
              "      <td>0.124914</td>\n",
              "      <td>0.307382</td>\n",
              "      <td>0.104777</td>\n",
              "      <td>0.103656</td>\n",
              "      <td>0.404495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>962</th>\n",
              "      <td>962.0</td>\n",
              "      <td>9.646530e-11</td>\n",
              "      <td>-0.093319</td>\n",
              "      <td>-0.032585</td>\n",
              "      <td>0.351276</td>\n",
              "      <td>-0.047832</td>\n",
              "      <td>0.227644</td>\n",
              "      <td>0.026683</td>\n",
              "      <td>-0.250957</td>\n",
              "      <td>0.138775</td>\n",
              "      <td>0.124914</td>\n",
              "      <td>0.307388</td>\n",
              "      <td>0.104775</td>\n",
              "      <td>0.103658</td>\n",
              "      <td>0.404495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>963 rows × 15 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9bd32b6-e19e-49d6-88ad-86ec44bb3ec0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a9bd32b6-e19e-49d6-88ad-86ec44bb3ec0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a9bd32b6-e19e-49d6-88ad-86ec44bb3ec0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pipeline"
      ],
      "metadata": {
        "id": "BBZcu0CaJuug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,4):\n",
        "\n",
        "  steps = [(\"imp_mean\", SimpleImputer()), \n",
        "           (\"scaler\", StandardScaler()),\n",
        "           (\"transform\", PolynomialFeatures(degree = i)),\n",
        "           (\"liReg\", LinearRegression())]\n",
        "\n",
        "  pipeline = Pipeline(steps)\n",
        "  pipeline.fit(X_train, Y_train)\n",
        "\n",
        "  rscore_test = pipeline.score(X_test, Y_test)\n",
        "  rscore_train = pipeline.score(X_train, Y_train)\n",
        "\n",
        "  Y_pred = pipeline.predict(X_test)\n",
        "\n",
        "  print(i, \"test: \", rscore_test, \"train: \", rscore_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbObncjE89O7",
        "outputId": "e83acc24-eb60-4045-bb82-ea2fb1a6a7de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 test:  0.5957198024266375 train:  0.5942977317317192\n",
            "2 test:  0.6617413060726104 train:  0.6774945138296509\n",
            "3 test:  -0.43720038016019735 train:  0.7129296381243041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###cv_scores"
      ],
      "metadata": {
        "id": "YWyE6NQ6KFLK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L567mYy4tvqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117e929b-082b-46b6-f2bf-52d82b18df80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.61811525 0.59336534 0.56422316 0.54112621 0.56151695 0.60915203\n",
            " 0.60434977 0.61398196 0.6016554  0.57755786]\n"
          ]
        }
      ],
      "source": [
        "print(cv_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###r2score_Linear Regression"
      ],
      "metadata": {
        "id": "874rTbT1KUjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "normalize = MinMaxScaler()\n",
        "X_scaled = normalize.fit_transform(X)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_scaled, Y)\n",
        "print(lr.coef_, lr.intercept_, \"\\n\")\n",
        "\n",
        "Y_pred = lr.predict(X_scaled)\n",
        "rscore = r2_score(Y, Y_pred)\n",
        "mse = mean_squared_error(Y, Y_pred)\n",
        "mae = mean_absolute_error(Y, Y_pred)\n",
        "\n",
        "print('r2-score:', rscore, '\\nmean squared error:', mse, '\\nmean absolute error:', mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ9mifMWKUBb",
        "outputId": "d3b81b70-4eef-44d9-92ae-54bdf777f80f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-8.55075149e-01 -3.20912504e-01  1.22377905e+12 -1.27921078e+00\n",
            " -6.43095303e+13  7.87967422e+12  6.43095303e+13  1.55618387e+00\n",
            "  6.48095791e-01  2.62153127e+00 -6.61715053e+12 -3.75673328e+12] -1.8917846679687502 \n",
            "\n",
            "r2-score: 0.5953707541736983 \n",
            "mean squared error: 0.4046042687123618 \n",
            "mean absolute error: 0.4179250175322381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nactual Price:\",Y[2], \"Predict Price:\",Y_pred[2], \"Differece:\", Y[2]-Y_pred[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OnJtebfIkRn",
        "outputId": "7e5ca40e-d4e2-4255-e043-271412bcd6e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "actual Price: -0.320283212536391 Predict Price: -0.4389038085937502 Differece: 0.1186205960573592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization for Linear Regression model"
      ],
      "metadata": {
        "id": "5yIXhfprFXHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import sklearn\n",
        "\n",
        "# df = df.head(16200)\n",
        "\n",
        "# # x = [df[:,1]].values.reshape(-1,1)\n",
        "\n",
        "# sns.scatterplot(data = df, x = 'sqft_living', y = 'price')\n",
        "# X_model = np.linspace(df['sqft_living'].min(), df['sqft_living'].max(), 50)\n",
        "# Y_model = lr.predict(sklearn.preprocessing.StandardScaler().fit_transform(X_model.reshape(-1,1)))\n",
        "\n",
        "# plt.plot(X_model,  Y_model, color = 'k')\n",
        "# plt.legend(['training data', 'predicted model'])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "_uo2lFYLKpJs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Polynomial Regression"
      ],
      "metadata": {
        "id": "4iMq6Y4QNHEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = df['sqft_living'].values.reshape(-1, 1)\n",
        "Y = df['price'].values.reshape(-1, 1)\n",
        "\n",
        "#Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X.reshape(-1,1))\n",
        "\n",
        "# Poly transformation\n",
        "order = 15\n",
        "poly = PolynomialFeatures(degree=order, include_bias = False)\n",
        "poly_features = poly.fit_transform(X_scaled.reshape(-1, 1))\n",
        "\n",
        "# Modeling. \n",
        "lr = LinearRegression()\n",
        "model = lr.fit(poly_features, Y)\n",
        "print(model.coef_, model.intercept_)\n",
        "\n",
        "#Prediction\n",
        "Y_pred = model.predict(poly_features)\n",
        "rscore = model.score(poly_features, Y)\n",
        "mse = mean_squared_error(Y, Y_pred)\n",
        "mae = mean_absolute_error(Y, Y_pred)\n",
        "\n",
        "print('r2-score:', rscore.round(2), '\\nmean squared error:', mse.round(2), '\\nmean absolute error:', mae.round(2))\n",
        "print(\"\")\n",
        "print(\"actual Price:\",Y[0], \"Predict Price:\",Y_pred[0], \"Differece:\", Y[0]-Y_pred[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOjeEE6nNGiq",
        "outputId": "a8a22772-35f8-42e6-8f96-a8a00cb8009c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.95312877e+05  9.66765981e+04 -2.42738598e+04 -6.06992588e+04\n",
            "   3.65979561e+04  1.72600879e+04 -1.52015408e+04  1.01970475e+03\n",
            "   1.77882682e+03 -6.94249225e+02  1.21436740e+02 -1.14791442e+01\n",
            "   5.66921396e-01 -1.11900635e-02 -2.12089653e-05]] [489886.28079857]\n",
            "r2-score: 0.56 \n",
            "mean squared error: 61357048663.92 \n",
            "mean absolute error: 165903.62\n",
            "\n",
            "actual Price: [350000.] Predict Price: [528681.88445033] Differece: [-178681.88445033]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression"
      ],
      "metadata": {
        "id": "VqsaOMxNHGfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pr = PolynomialFeatures(degree = i)\n",
        "\n",
        "X_train_pr = pr.fit_transform(X_train)\n",
        "X_test_pr = pr.fit_transform(X_test)\n",
        "\n",
        "ridgeReg = Ridge(alpha=0.001, normalize=True)\n",
        "ridgeReg.fit(X_train_pr, Y_train)\n",
        "\n",
        "Ypred_train = ridgeReg.predict(X_train_pr)\n",
        "Ypred_test = ridgeReg.predict(X_test_pr)\n",
        "\n",
        "ridgeReg.score(X_train_pr, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOd_k7q5HF01",
        "outputId": "0ea90391-dbc5-49cb-e2ab-aceed27eb1c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.712232360565002"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso Regression"
      ],
      "metadata": {
        "id": "OhMg-o-1HSMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pr = PolynomialFeatures(degree = i)\n",
        "X_train_pr = pr.fit_transform(X_train)\n",
        "X_test_pr = pr.fit_transform(X_test)\n",
        "\n",
        "lassoReg = Lasso(alpha=100, max_iter=20000, normalize=True)\n",
        "lassoReg.fit(X_train_pr, Y_train)\n",
        "\n",
        "Ypred_train = lassoReg.predict(X_train_pr)\n",
        "Ypred_test = lassoReg.predict(X_test_pr)\n",
        "\n",
        "lassoReg.score(X_train_pr, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41HXLVOOHRwD",
        "outputId": "7a57815e-4266-46c1-b82d-ba881dd825ab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cv_score of Ridge and Lasso Regression"
      ],
      "metadata": {
        "id": "2krRIuDmIAe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "kfold = KFold(n_splits= 10, shuffle=True, random_state=100)\n",
        "\n",
        "# lr = LinearRegression()\n",
        "\n",
        "steps = [(\"imp_mean\", SimpleImputer()),\n",
        "          (\"scaler\" , StandardScaler()),\n",
        "          (\"tranform\" , PolynomialFeatures(degree = i)),\n",
        "          (\"rReg\" , Ridge(alpha=0, normalize=True)),\n",
        "          # (\"lRag\", Lasso(alpha=100, max_iter=20000, normalize=True))\n",
        "          ]\n",
        "          \n",
        "# r=Ridge(alpha=100, normalize=True)\n",
        "# l=Lasso(alpha=100, max_iter=20000, normalize=True)\n",
        "\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "cv_scores = cross_val_score(pipeline, X_train, Y_train, cv=kfold)\n",
        "\n",
        "print(cv_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwuN8NIVH_Rl",
        "outputId": "eec507b2-845e-43ad-b77a-9d4d5b742ffb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.65076001 0.73817346 0.53201946 0.55748443 0.6203095  0.64939624\n",
            " 0.65056471 0.56987742 0.57242003 0.59159392]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization for Poly regression"
      ],
      "metadata": {
        "id": "TtWYH4N8IbC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sns.scatterplot(data = df, x = 'sqft_living', y = 'price')\n",
        "#x_model = np.linspace(df['sqft_living'].min(), df['sqft_living'].max(), 50)\n",
        "#x_model_scaled = scaler.fit_transform(x_model.reshape(-1,1))\n",
        "\n",
        "#x_model_poly  = poly.fit_transform(x_model_scaled)\n",
        "#y_model = model.predict(x_model_poly)\n",
        "\n",
        "#plt.plot(x_model,  y_model, color = 'k')\n",
        "#plt.legend(['training data', 'predicted model with degree ' + str(order)])\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "Zd_6wP9vIalx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib \n",
        "X = df['sqft_living'].values \n",
        "y = df['price'].values\n",
        "\n",
        "scaler = StandardScaler() \n",
        "X_scaled = scaler.fit_transform(X.reshape(-1,1))\n",
        "\n",
        "poly = PolynomialFeatures(degree=6, include_bias=False) \n",
        "poly_features = poly.fit_transform(X_scaled.reshape(-1, 1))\n",
        "\n",
        "lr = LinearRegression() \n",
        "model = lr.fit(poly_features, y)\n",
        "\n",
        "joblib.dump(model,'best_model.pkl')\n",
        "\n",
        "Y_pred = model.predict(poly_features) \n",
        "rscore = model.score(poly_features, y) \n",
        "mse = mean_squared_error(y, Y_pred) \n",
        "mae = mean_absolute_error(y, Y_pred)"
      ],
      "metadata": {
        "id": "BW1nr8m1JQAh"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ebfc0a8d552866b0d59eba665220a57de3bc06f3ac643b8bef38dd8f66781fdd"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}