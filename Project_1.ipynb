{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "BQZ3yvF3tvqI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df=pd.read_csv('house_data_train.csv', index_col = 0)\n",
        "df = df.dropna()\n",
        "df1 = df.head(15000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6JgjFkHtvqJ"
      },
      "source": [
        "### Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "L567mYy4tvqK"
      },
      "outputs": [],
      "source": [
        "from numpy.lib import polynomial\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "X = df1['sqft_living'].values.reshape(-1,1)\n",
        "y = df1['price'].values.reshape(-1,1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle=False)\n",
        "\n",
        "kfold = KFold(n_splits = 10, shuffle = True, random_state = 100)\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "cv_scores = cross_val_score(lr, X_train, y_train, cv = kfold)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression"
      ],
      "metadata": {
        "id": "hp1kSYNKWuF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# # import math\n",
        "\n",
        "# # import pandas as pd\n",
        "# # import numpy as np\n",
        "\n",
        "# def initialize(dim):\n",
        "#     b=random.random()\n",
        "#     a=np.random.rand(dim)\n",
        "#     return b,a\n",
        "\n",
        "# def predict_Y(b,a,X):\n",
        "#     return b + np.dot(X,a)\n",
        "\n",
        "# def get_cost(Y,Y_hat):\n",
        "#     Y_resd=Y-Y_hat\n",
        "#     return np.sum(np.dot(Y_resd.T,Y_resd))/len(Y-Y_resd)\n",
        "\n",
        "# def update_theta(x,y,y_hat,b_0,theta_o,learning_rate):\n",
        "#     db=(np.sum(y_hat-y)*2)/len(y)\n",
        "#     dw=(np.dot((y_hat-y),x)*2)/len(y)\n",
        "#     b_1=b_0-learning_rate*db\n",
        "#     theta_1=theta_o-learning_rate*dw\n",
        "#     return b_1,theta_1\n",
        "\n",
        "# def run_gradient_descent(X,Y,alpha,num_iterations):\n",
        "#     tolerance = 1e-8\n",
        "#     b,theta=initialize(X.shape[1])\n",
        "#     num_weight = len('sqft_living')\n",
        "#     weights = [f'weights_a{i}' for i in range(num_weight)]\n",
        "#     gd_iterations_df=pd.DataFrame(columns=['iteration','value_b', *weights, 'cost'])\n",
        "#     result_idx=0\n",
        "#     prev_cost = 0\n",
        "#     for iter_num in range(num_iterations):\n",
        "#         Y_hat=predict_Y(b,theta,X)\n",
        "#         this_cost=get_cost(Y,Y_hat)\n",
        "#         prev_b=b\n",
        "#         prev_theta=theta\n",
        "#         b,theta=update_theta(X,Y,Y_hat,prev_b,prev_theta,alpha)\n",
        "#         gd_iterations_df.loc[result_idx]=[iter_num,b, *theta[:num_weight] , this_cost]\n",
        "#         result_idx=result_idx+1   \n",
        "#         if (np.abs(this_cost - prev_cost)<tolerance):\n",
        "#           print(np.abs(this_cost - prev_cost))\n",
        "#           break\n",
        "#         prev_cost = this_cost\n",
        "#     return gd_iterations_df,b,theta\n",
        "        \n",
        "# gd_iterations_df,b,theta=run_gradient_descent(X,y,alpha=0.01,num_iterations=1000)\n",
        "# #gd_iterations_df.head(5)\n",
        "# gd_iterations_df"
      ],
      "metadata": {
        "id": "PwPFvrRrWpsB"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pipeline_Linear Regression"
      ],
      "metadata": {
        "id": "qsy9gBIk0oyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "for i in range(1,20):\n",
        "\n",
        "  steps = [(\"imp_mean\", SimpleImputer()), \n",
        "           (\"scaler\", StandardScaler()),\n",
        "           (\"transform\", PolynomialFeatures(degree = i)),\n",
        "           (\"liReg\", LinearRegression())]\n",
        "\n",
        "  pipeline = Pipeline(steps)\n",
        "  \n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  rscore_test = pipeline.score(X_test, y_test)\n",
        "  rscore_train = pipeline.score(X_train, y_train)\n",
        "\n",
        "  y_pred = pipeline.predict(X_test)\n",
        "\n",
        "  print(i, \"test: \", rscore_test, \"train: \", rscore_train)"
      ],
      "metadata": {
        "id": "NX11yb3R0n1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d01fcd2-c06a-40e6-8de0-4323dd496ed1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 test:  0.5120760466376741 train:  0.4869329578578002\n",
            "2 test:  0.603690516166391 train:  0.5345402684747389\n",
            "3 test:  0.6039636817090663 train:  0.5345494669644933\n",
            "4 test:  0.592332942422171 train:  0.5357647599809185\n",
            "5 test:  0.3102347431635679 train:  0.5395923677958909\n",
            "6 test:  0.3405835214423504 train:  0.5395950517604571\n",
            "7 test:  0.31114445740491214 train:  0.5395953481063513\n",
            "8 test:  -5.7702059086529855 train:  0.5397562371829467\n",
            "9 test:  -224.2356496216139 train:  0.5404616488420089\n",
            "10 test:  -1193.5509341071709 train:  0.5406348326575674\n",
            "11 test:  -1073.6533275127547 train:  0.5407976763802123\n",
            "12 test:  -27179.618683148838 train:  0.5408997467470734\n",
            "13 test:  -10724449.298804343 train:  0.5426525194803484\n",
            "14 test:  -251689141.7617726 train:  0.5444506537609398\n",
            "15 test:  -2091749442.7149615 train:  0.5451271781812381\n",
            "16 test:  -9279335336.854696 train:  0.5452586844937665\n",
            "17 test:  -9571655047.87511 train:  0.5450868444603723\n",
            "18 test:  -431504785003.5421 train:  0.542899052835304\n",
            "19 test:  -64933627432396.055 train:  0.45428995653220017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Print cv_scores for Linear Regression"
      ],
      "metadata": {
        "id": "3d-kim3v1sKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cv_scores)"
      ],
      "metadata": {
        "id": "5VyDqWaA1m_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0cc1e5-e358-46d6-be3f-3ee7d7716422"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.49192709 0.48496681 0.4817647  0.44957235 0.49459868 0.48072631\n",
            " 0.51472523 0.47630307 0.48177975 0.49461894]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### r2score_Linear Rregression"
      ],
      "metadata": {
        "id": "ru9llir4DyU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "normalize = MinMaxScaler()\n",
        "X_scaled = normalize.fit_transform(X)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_scaled, y)\n",
        "print(lr.coef_, lr.intercept_, \"\\n\")\n",
        "\n",
        "y_pred = lr.predict(X_scaled)\n",
        "rscore = r2_score(y, y_pred)\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "\n",
        "print('r2-score:', rscore, '\\nmean squared error:', mse, '\\nmean absolute error:', mae)"
      ],
      "metadata": {
        "id": "MZV8MOAIB4MW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64766635-8e16-4167-98f0-2079b4bc4ce0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3363685.95159992]] [49431.20170909] \n",
            "\n",
            "r2-score: 0.49762406833852624 \n",
            "mean squared error: 71332882243.79005 \n",
            "mean absolute error: 176205.98724291366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization for Linear Regression model"
      ],
      "metadata": {
        "id": "hB53snnCUDma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "#import sklearn\n",
        "\n",
        "#sns.scatterplot(data = df1, x = 'sqft_living', y = 'price')\n",
        "#x_model = np.linspace(df1['sqft_living'].min(), df1['sqft_living'].max(), 50)\n",
        "#y_model = lr.predict(sklearn.preprocessing.StandardScaler().fit_transform(x_model.reshape(-1,1)))\n",
        "\n",
        "# y_model = lr.predict(scaler.fit_transform(x_model.reshape(-1,1)))\n",
        "# X = sklearn.preprocessing.StandardScaler().fit_transform(X)\n",
        "\n",
        "#plt.plot(x_model,  y_model, color = 'k')\n",
        "#plt.legend(['training data', 'predicted model'])\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "la2sxzZ_UBd9"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial regression :\n",
        "1) Feature scaling and Transform poly features \n",
        "2) model using a linear regression. \n",
        "3) Predict using the Model "
      ],
      "metadata": {
        "id": "p0K6qngo8mkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "#Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X.reshape(-1,1))\n",
        "\n",
        "# Poly transformation\n",
        "order = 15\n",
        "poly = PolynomialFeatures(degree=order, include_bias = False)\n",
        "poly_features = poly.fit_transform(X_scaled.reshape(-1, 1))\n",
        "\n",
        "# Modeling. \n",
        "lr = LinearRegression()\n",
        "model = lr.fit(poly_features, y)\n",
        "print(model.coef_, model.intercept_)\n",
        "\n",
        "#Prediction\n",
        "Y_pred = model.predict(poly_features)\n",
        "rscore = model.score(poly_features, y)\n",
        "mse = mean_squared_error(y, Y_pred)\n",
        "mae = mean_absolute_error(y, Y_pred)\n",
        "\n",
        "print('r2-score:', rscore.round(2), '\\nmean squared error:', mse.round(2), '\\nmean absolute error:', mae.round(2))\n",
        "print(\"\")\n",
        "print(\"actual Price:\",y[2], \"Predict Price:\",Y_pred[2], \"Differece:\", y[2]-Y_pred[2])"
      ],
      "metadata": {
        "id": "l847qtcB22K5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595b3457-f8c9-4349-a99c-aac5ae13ef08"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.94957184e+05  8.90350993e+04 -2.06052532e+04 -4.79100168e+04\n",
            "   3.12227477e+04  1.19194656e+04 -1.19292638e+04  1.15762445e+03\n",
            "   1.24987435e+03 -5.22477056e+02  9.53816049e+01 -9.50541165e+00\n",
            "   5.14424927e-01 -1.28929215e-02  7.62588752e-05]] [490746.55291925]\n",
            "r2-score: 0.57 \n",
            "mean squared error: 61577100114.73 \n",
            "mean absolute error: 166039.7\n",
            "\n",
            "actual Price: [422000.] Predict Price: [374651.45326817] Differece: [47348.54673183]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ridge Regression"
      ],
      "metadata": {
        "id": "2RRSpW5oMO_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pr=PolynomialFeatures(degree = i)\n",
        "\n",
        "X_train_pr = pr.fit_transform(X_train)\n",
        "X_test_pr = pr.fit_transform(X_test)\n",
        "\n",
        "ridgeReg = Ridge(alpha=0.001, normalize=True)\n",
        "ridgeReg.fit(X_train_pr, y_train)\n",
        "\n",
        "Ypred_train = ridgeReg.predict(X_train_pr)\n",
        "Ypred_test = ridgeReg.predict(X_test_pr)\n",
        "\n",
        "ridgeReg.score(X_train_pr, y_train)"
      ],
      "metadata": {
        "id": "wiw5miG_HFJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a19ba2dc-2e72-4ca3-b272-4734e859f903"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5388534047119149"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lasso Regression"
      ],
      "metadata": {
        "id": "EMOKqt9TMXcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pr=PolynomialFeatures(degree = 3)\n",
        "X_train_pr = pr.fit_transform(X_train)\n",
        "X_test_pr = pr.fit_transform(X_test)\n",
        "\n",
        "lassoReg = Lasso(alpha=0.01, max_iter=20000, normalize=True)\n",
        "lassoReg.fit(X_train_pr, y_train)\n",
        "\n",
        "Ypred_train = lassoReg.predict(X_train_pr)\n",
        "Ypred_test = lassoReg.predict(X_test_pr)\n",
        "\n",
        "lassoReg.score(X_train_pr, y_train)\n",
        "\n",
        "print(\"actual Price:\",y[0], \"Predict Price:\",Ypred_train[0], \"Differece:\", y[0]-Ypred_train[0])"
      ],
      "metadata": {
        "id": "dCgUtM2hI-Zj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70bee537-b818-40c3-e492-7259a7050697"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual Price: [350000.] Predict Price: 537900.3137392905 Differece: [-187900.31373929]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###cv_score for Ridge and Lasso Regression"
      ],
      "metadata": {
        "id": "ldb0EaupVPHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "kfold = KFold(n_splits= 10, shuffle=True, random_state=100)\n",
        "\n",
        "# lr = LinearRegression()\n",
        "\n",
        "steps = [(\"imp_mean\", SimpleImputer()),\n",
        "          (\"scaler\" , StandardScaler()),\n",
        "          (\"tranform\" , PolynomialFeatures(degree = i)),\n",
        "          # (\"rReg\" , Ridge(alpha=0, normalize=True)),\n",
        "          (\"lRag\", Lasso(alpha=100, max_iter=20000, normalize=True))\n",
        "          ]\n",
        "          \n",
        "# r=Ridge(alpha=100, normalize=True)\n",
        "# l=Lasso(alpha=100, max_iter=20000, normalize=True)\n",
        "\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=kfold)\n",
        "\n",
        "print(cv_scores)"
      ],
      "metadata": {
        "id": "Q7Tn9rSnMZzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba45d52-571c-413d-ccd9-ab2f94ef616e"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.52960694 0.51834565 0.59814104 0.49671377 0.52536056 0.49239117\n",
            " 0.58936069 0.50695351 0.48863771 0.53799843]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvXhDoHAtvqM"
      },
      "source": [
        "### Visualization for Poly regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "0ItUg2oZtvqM"
      },
      "outputs": [],
      "source": [
        "#sns.scatterplot(data = df1, x = 'sqft_living', y = 'price')\n",
        "#x_model = np.linspace(df1['sqft_living'].min(), df1['sqft_living'].max(), 50)\n",
        "#x_model_scaled = scaler.fit_transform(x_model.reshape(-1,1))\n",
        "\n",
        "#x_model_poly  = poly.fit_transform(x_model_scaled)\n",
        "#y_model = model.predict(x_model_poly)\n",
        "\n",
        "#plt.plot(x_model,  y_model, color = 'k')\n",
        "#plt.legend(['training data', 'predicted model with degree ' + str(order)])\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWmHQOUntvqM"
      },
      "source": [
        "### Saving Best Model for Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VChBLl4tvqM"
      },
      "source": [
        "import joblib\n",
        "X = df1['sqft_living'].values\n",
        "y = df1['price'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X.reshape(-1,1))\n",
        "\n",
        "poly = PolynomialFeatures(degree=best_order, include_bias=False)\n",
        "poly_features = poly.fit_transform(X_scaled.reshape(-1, 1))\n",
        "    \n",
        "lr = LinearRegression()\n",
        "model = lr.fit(poly_features, y)\n",
        "\n",
        "joblib.dump(model,'best_model.pkl')\n",
        "\n",
        "Y_pred = model.predict(poly_features)\n",
        "rscore = model.score(poly_features, y)\n",
        "mse = mean_squared_error(y, Y_pred)\n",
        "mae = mean_absolute_error(y, Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib \n",
        "X = df1['sqft_living'].values \n",
        "y = df1['price'].values\n",
        "\n",
        "scaler = StandardScaler() \n",
        "X_scaled = scaler.fit_transform(X.reshape(-1,1))\n",
        "\n",
        "# poly = PolynomialFeatures(degree=6, include_bias=False) \n",
        "# poly_features = poly.fit_transform(X_scaled.reshape(-1, 1))\n",
        "\n",
        "# lr = LinearRegression() \n",
        "# model = lr.fit(poly_features, y)\n",
        "\n",
        "pr=PolynomialFeatures(degree = 3)\n",
        "X_train_pr = pr.fit_transform(X_train)\n",
        "X_test_pr = pr.fit_transform(X_test)\n",
        "\n",
        "lassoReg = Lasso(alpha=0.01, max_iter=20000, normalize=True)\n",
        "lassoReg.fit(X_train_pr, y_train)\n",
        "\n",
        "Ypred_train = lassoReg.predict(X_train_pr)\n",
        "Ypred_test = lassoReg.predict(X_test_pr)\n",
        "\n",
        "joblib.dump(model,'best_model.pkl')\n",
        "\n",
        "Y_pred = model.predict(poly_features) \n",
        "rscore = model.score(poly_features, y) \n",
        "mse = mean_squared_error(y, Y_pred) \n",
        "mae = mean_absolute_error(y, Y_pred)"
      ],
      "metadata": {
        "id": "o-Q3wxEPuODS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f66af4a-abd0-436c-cadc-f00675353326"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ebfc0a8d552866b0d59eba665220a57de3bc06f3ac643b8bef38dd8f66781fdd"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}